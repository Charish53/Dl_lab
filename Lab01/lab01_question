Ex No. 1			Feed Forward & Back-Propagation Learning Algorithm
Date: 20-01-2025

NOTE - Do not use inbuilt functions for perceptron.

Implement the Perceptron algorithm from scratch in Python.
Initialize the weights with [0 0 0] and a learning rate of 0.0001.
For each iteration, calculate the output of the Perceptron for each input in the training set.
Use MSE to computer the error for all samples
Update the weights using the gradient descent procedure.
Repeat the above steps until the Perceptron converges or a maximum number of iterations is reached.
Test the trained Perceptron on a separate test set, explain how you came up with the test set.
Use the step function as an  activation function in the output layer

Use the IRIS Dataset for the above, considering all four features: sepal length, sepal width, petal length, and petal width, but only two classes -  Setosa, and Versicolor.  Drop the feature vectors of the other class. 
Please find the dataset here - Iris Dataset

Implement the feedforward and backpropagation learning algorithm for multiple perceptrons in Python for the question provided in the attached image.
Initialize the weights and biases randomly.
Implement the forward pass.
Compute the loss between the predicted output and the actual output using an appropriate loss function.
Compute the gradients of the loss function with respect to the weights and biases using the chain rule.
Update the weights and biases.
Iterate over multiple times (epochs), performing forward propagation, loss calculation, backpropagation, and parameter updates in each iteration till convergence.
